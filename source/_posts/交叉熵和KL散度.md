---
title: 交叉熵和负对数损失
toc: true
date: 2020-03-17 10:27:38
tags:
categories:
- 深度学习
- 基础理论
---

最近学的有点迷糊。复习一下各种常用的损失函数的公式。
<!--more-->
# 交叉熵
交叉熵主要用于分类问题。用于比较两组数据的分布差异。
交叉熵避免了使用二次代价函数带来的梯度消失问题（[消除了sigmoid的导数项](https://juejin.im/post/5b40a5156fb9a04faf478a45)）。
## 单分类问题的loss
一个样本只能有一个类别
一个样本的loss
$$loss=-\sum_{i=1}^{n}{y_i\log(\hat{y_i})}$$
batch的loss(m为batch size)
$$loss=-\frac{1}{m}\sum_{j=1}^{m}\sum_{i=1}^{n}{y_{ji}\log(\hat{y_{ji}})}$$

## 多标签问题的loss
一个样本可以有多个类别。n-hot
Pred采用的是sigmoid函数计算。将每一个节点的输出归一化到[0,1]之间。所有Pred值的和也不再为1。
一个样本的loss：
$$loss=-ylog(\hat{y})-(1-y)log(1-\hat{y})$$
batch的loss
$$loss=\sum_{j=1}^{m}\sum_{i=1}^{m}{-y_{ji}log{\hat{y_{ji}}}-(1-y_{ji})log(1-\hat{y_{ji}})}$$

# KL散度
KL散度是信息论中的重要概念，用来描述两个概率分布的差异。

如果对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，则可以使用KL散度(Kullback-Leiber(KL) divergence)来衡量这两个分布的差异：

$$D_{KL}(P||Q) = E_{x\sim P}[log\frac{P(x)}{Q(x)}] = E_{x\sim P}[log{P(x)}-log{Q(x)}]$$

KL散度有很多有用的性质：
- 最重要的是，它是非负的。 
- 当x是离散型变量时，KL散度为0当且仅当P(x)与Q(x)具有相同的分布。
- KL散度表征了某种距离，但不是真正的距离，因为KL散度不对称：$D_{KL}(P||Q)\not=D_{KL}(Q||P)$

# 交叉熵和KL散度
> KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价。

交叉熵与KL散度密切相关：
$$H(P,Q) = H(P) + D_{KL}(P||Q)$$
其中H(P,Q)是交叉熵(cross-entropy)，H(P)是概率分布P的香农熵。
在深度学习中，针对Q最小化交叉熵等价于最小化KL散度，因为Q与P独立，Q与P的香农熵无关。


# 交叉熵和负对数损失（negative log likelihood，NLL)
[参考链接](https://stats.stackexchange.com/questions/198038/cross-entropy-or-log-likelihood-in-output-layer)
具有交叉熵的S形(sigmoid激活，每一个输出具有独立分布)输出层与具有对数似然的softmax（输出和为1）输出层非常相似。

考虑使用以下分类任务(K类)。
让我们分别看一下网络的输出层和成本函数。就我们的目的而言，输出层是S形或softmax，成本函数是交叉熵或对数似然。
**输出层**
如果是S型，输出层将具有𝐾每个Sigmoid的值在0到1之间。至关重要的是，这些输出的总和可能不等于1，因此不能将它们解释为概率分布。这两个语句的唯一例外是以下情况𝐾=2即二分类，此时只有一个S形就足够了。在这种情况下，第二类的预测值可以由1减去输出的预测值得到。
如果输出层是softmax，则它还具有𝐾输出。但是在这种情况下，输出之和为1。由于这一限制，具有softmax输出层的网络比具有多个S型网络的网络具有更低的灵活性。
为了说明该约束，请考虑用于对数字进行分类的网络。它有十个输出节点。如果它们是S形，则它们中的两个（例如8和9或0和6）都可以输出（例如0.9）。对于softmax，这是不可能的。输出仍然可以相等（例如均为0.45），但是由于受到限制，当调整权重以增加一位数字的输出时，它必然会降低其他位数的输出。该文本在同一章中有一个滑块演示来说明这种效果。
那预测呢？好吧，一种简单的方法是简单地分配具有最大输出的类。对于两种类型的输出层都是如此。
至于代价函数，可以对任何一个网络使用交叉熵或对数似然（或其他成本函数，例如均方误差）
**代价函数**
K分类问题的交叉熵损失为：
$$C_\text{CE} = -\frac{1}{n} \sum\limits_x \sum\limits_{k=1}^K (y_k \ln a_k^L + (1 - y_k) \ln (1 - a_k^L))$$
这里x是输入(对于每个x，只有$y_k$为1，其余为0（即one-hot编码）)， n是input size
K分类的NLL损失为：
$$C_\text{LL} = -\frac{1}{n} \sum\limits_x y^T \ln(a^L) = -\frac{1}{n} \sum\limits_x \sum\limits_{k=1}^K y_k \ln(a_k^L)$$
这里，y是one-hot编码的输出，并且$a^L$是模型的输出
这是两个成本函数之间的关键区别：对数似然仅考虑相应类别的输出，而交叉熵函数也考虑其他输出。您可以在上面的表达式中看到这一点-总而言之CE和CLL有相同的项，但是CE还有一个附加项。这意味着CE和LL都以正确类别的输出量奖励网络。但是，CE还对网络中其他类别的输出量进行了惩罚。如果混乱很严重，那么惩罚也很严重。
> 总而言之，如果要考虑其他类别输出量的影响就使用CE，如果不考虑就使用CLL

