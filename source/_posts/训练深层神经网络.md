---
title: 训练深层神经网络
toc: true
date: 2020-02-04 23:39:38
tags: [深度学习]
categories:
- 深度学习
- 基础理论
---

# 梯度消失/爆炸问题
<!--more-->
Sigmoid函数，当输入变大（负或正）时，函数饱和在 0 或 1，导数非常接近 0。因此，当反向传播开始时， 它几乎没有梯度通过网络传播回来，而且由于反向传播通过顶层向下传递，所以存在的小梯度不断地被稀释，因此较低层确实没有任何东西可用。
Glorot 和 Bengio 在他们的论文中提出了一种显著缓解这个问题的方法。 我们需要信号在两个方向上正确地流动：在进行预测时是正向的，在反向传播梯度时是反向的。 我们不希望信号消失，也不希望它爆炸并饱和。 为了使信号正确流动，作者认为，我们需要每层输出的方差等于其输入的方差。
Xavier初始化:
默认情况下，`dense()`函数使用 Xavier 初始化（具有统一的分布）。可以`variance_scaling_initializer()`函数来将其更改为 He 初始化。

# 非饱和激活函数
ELU > leaky ReLU(及其变体) > ReLU > tanh > sigmoid
如果您关心运行时性能，那么您可能喜欢 leaky ReLU超过ELU。 如果你不想调整另一个超参数，你可以使用前面提到的默认的`α`值（leaky ReLU 为 0.01，ELU 为 1）。 如果您有充足的时间和计算能力，您可以使用交叉验证来评估其他激活函数，特别是如果您的神经网络过拟合，则为RReLU; 如果您拥有庞大的训练数据集，则为 PReLU。

# 批量标准化
`batch_norm()`
标准化：均值为0方差为1

# 优化器
动量优化，Nesterov 加速梯度，AdaGrad，RMSProp，Adam 优化
总是应该使用`Adam_optimization`

# 正则化避免过拟合

# 数据增强

# 配置
![](t-11-2.png)

这个默认配置可能需要调整：

*   如果你找不到一个好的学习率（收敛速度太慢，所以你增加了训练速度，现在收敛速度很快，但是网络的准确性不是最理想的），那么你可以尝试添加一个学习率调整，如指数衰减。
*   如果你的训练集太小，你可以实现数据增强。
*   如果你需要一个稀疏的模型，你可以添加 l1 正则化混合（并可以选择在训练后将微小的权重归零）。 如果您需要更稀疏的模型，您可以尝试使用 FTRL 而不是 Adam 优化以及 l1 正则化。
*   如果在运行时需要快速模型，则可能需要删除批量标准化，并可能用 leakyReLU 替换 ELU 激活函数。 有一个稀疏的模型也将有所帮助。
