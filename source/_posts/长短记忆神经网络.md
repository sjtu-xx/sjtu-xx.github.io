---
title: 长短记忆神经网络
toc: true
date: 2020-02-17 21:47:12
tags: [RNN,LSTM,神经网络]
categories:
- 深度学习
- 基础理论
---

理解LSTM：[原文链接](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) [中文链接](https://www.jianshu.com/p/95d5c461924c)
<!--more-->
# 循环神经网络（RNN）
循环神经网络通过将信息循环操作，保证信息持续存在。

# 长依赖存在的问题
存在两个问题：
- 预测"the cloud in the sky"中的sky，只需要短距离的信息。
- 预测"I grew up in France… I speak fluent French."中的french就需要长距离的信息。

理论上RNNs是能够处理这种“长依赖”问题的。通过调参来解决这种问题。但是在实践过程中RNNs无法学习到这种特征。Hochreiter (1991) [German] 和Bengio, et al. (1994)深入研究过为什么RNNs没法学习到这种特征。

# LSTM网络

在标准的RNN中，该重复模块将具有非常简单的结构，例如单个tanh层。标准的RNN网络如下图所示
![](LSTM3-SimpleRNN.png)
LSTMs也具有这种链式结构，但是它的重复单元不同于标准RNN网络里的单元只有一个网络层，它的内部有四个网络层。LSTMs的结构如下图所示。
![](LSTM3-chain.png)
图中黄色类似于CNN里的激活函数操作，粉色圆圈表示点操作，单箭头表示数据流向，箭头合并表示向量的合并（concat）操作，箭头分叉表示向量的拷贝操作.

# LSTMs核心思想
LSTMs的核心是细胞状态，用贯穿细胞的水平线表示。
细胞状态像传送带一样。它贯穿整个细胞却只有很少的分支，这样能保证信息不变的流过整个RNNs。
LSTM网络能通过一种被称为门的结构对细胞状态进行删除或者添加信息。
门能够有选择性的决定让哪些信息通过。其实门的结构很简单，就是一个sigmoid层和一个点乘操作的组合。

# keras
将LSTM层的数据整形为(number_of_sequences, number_of_steps,features)。
即：（样本个数，时间步，特征个数）
对每一个样本，需要有number_of_steps个时间步的样本，每个时间步都要满足特征个数。
